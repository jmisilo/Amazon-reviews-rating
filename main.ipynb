{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d52ecdb6",
   "metadata": {},
   "source": [
    "## 1. Import depedencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4cb07cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import functools\n",
    "import operator \n",
    "import json\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from dataclasses import dataclass\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a34114",
   "metadata": {},
   "source": [
    "## 2. Data works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f52ef080",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "        self.vocab = {\n",
    "            '<unk>': 0,\n",
    "            '<pad>': 1,\n",
    "            '<sos>': 2,\n",
    "            '<eos>': 3\n",
    "        }\n",
    "        \n",
    "        self.build_vocab()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        assert type(index) in [str, int], 'Index type must be string or int'\n",
    "        \n",
    "        if isinstance(index, str):\n",
    "            try:\n",
    "                return self.vocab[index]\n",
    "            \n",
    "            except KeyError:\n",
    "                return self.vocab['<unk>']\n",
    "        \n",
    "        elif isinstance(index, int):\n",
    "            try:\n",
    "                return list(self.vocab.keys())[list(self.vocab.values()).index(index)]\n",
    "            except (KeyError,ValueError):\n",
    "                return self[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def append_word(self, word):\n",
    "        if not word in self.vocab:\n",
    "            self.vocab[word] = len(self)\n",
    "    \n",
    "    def build_vocab(self):\n",
    "        bag_of_words = sorted(list(set(self.data)))\n",
    "        \n",
    "        for word in bag_of_words:\n",
    "            self.append_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b906f444",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.path = 'data/Video_Games_5.json'\n",
    "        self.prep_path = 'data/preprocessed.csv'\n",
    "        \n",
    "        # if preprocessed data already exists - load it\n",
    "        if os.path.isfile(self.prep_path):\n",
    "            self.data = pd.read_csv(self.prep_path)[['reviewText', 'overall']]\n",
    "        \n",
    "        # else preprocess and save\n",
    "        else:\n",
    "            with open(self.path, 'r') as f:\n",
    "                lines = [json.loads(line.rstrip()) for line in f]\n",
    "                self.data = pd.DataFrame(lines)[['verified', 'reviewText', 'overall']]\n",
    "                self.data = self.data[self.data['verified']]\n",
    "\n",
    "            self.data = self.data.dropna()\n",
    "            self.data = self.data.reset_index(drop=True)\n",
    "            self.data = self.data.drop('verified', axis=1)\n",
    "\n",
    "            self.data['reviewText'] = self.data['reviewText'].apply(self.clean_data)\n",
    "            self.tok_lemma()\n",
    "\n",
    "            self.data.to_csv(self.prep_path)\n",
    "            \n",
    "        self.build_vocab()\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        assert type(index) == int, 'Index must be int'\n",
    "        \n",
    "        item = self.data.iloc[index]\n",
    "        text = str(item['reviewText']).split()\n",
    "        \n",
    "        for i, word in enumerate(text):\n",
    "            text[i] = self.Voc[word]\n",
    "            \n",
    "        return text, item['overall']\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_data(text):\n",
    "        if type(text) != str:\n",
    "            return '   '\n",
    "        \n",
    "        # lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # remove \\n signs\n",
    "        text = text.replace('\\n\\n\\n\\n', ' ').replace('\\n\\n\\n', ' ').replace('\\n\\n', ' ').replace('\\n', ' ')\n",
    "        \n",
    "        # remove url\n",
    "        text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # remove punctuations\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        return text \n",
    "    \n",
    "    def tok_lemma(self):\n",
    "        reviews = self.data['reviewText'].values\n",
    "\n",
    "        reviews = nlp.pipe(reviews, batch_size=128, n_process=3)\n",
    "\n",
    "        reviews = [' '.join([word.lemma_ for word in text if not word in stopwords and word.lemma_ and word.text.isalpha()]) for text in reviews]\n",
    "\n",
    "        self.data['reviewText'] = pd.Series(reviews) \n",
    "        \n",
    "    def build_vocab(self):\n",
    "        bag_of_words = self.data['reviewText'].apply(lambda x: str(x).split()).tolist()\n",
    "\n",
    "        bag_of_words = functools.reduce(operator.iconcat, bag_of_words, [])\n",
    "        \n",
    "        self.Voc = Vocabulary(bag_of_words)\n",
    "\n",
    "        self.Voc.build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b91f07a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = RevDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e734e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class cfg:\n",
    "    max_length = 310\n",
    "    embed_size = 120\n",
    "    hidden_size = 512\n",
    "    num_layers = 3\n",
    "    heads = 8\n",
    "    batch_size = 32\n",
    "    lr = 3e-4\n",
    "    vocab_size = len(data.Voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d381485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lens = [len(data[i][0]) for i in range(len(data)) if len(data[i][0]) < 500]\n",
    "# plt.figure(figsize=(11, 8))\n",
    "# sns.histplot(data=lens, bins=15, kde=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "958c6e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(batch):\n",
    "    reviews = []\n",
    "    overalls = []\n",
    "    for i, (text, overall) in enumerate(batch):\n",
    "        \n",
    "        text_len = len(text)\n",
    "        \n",
    "        if text_len == cfg.max_length:\n",
    "            pass\n",
    "        elif text_len > cfg.max_length:\n",
    "            text = text[:cfg.max_length]\n",
    "        else:\n",
    "            pad_len = cfg.max_length - text_len\n",
    "            for j in range(pad_len):\n",
    "                # 1 - index of <pad> token in Vocabulary\n",
    "                text.append(1)\n",
    "        reviews.append(torch.Tensor(text).type(torch.int64))\n",
    "        overalls.append(int(overall) - 1)\n",
    "    \n",
    "    overalls = torch.LongTensor(overalls)\n",
    "    overalls = F.one_hot(overalls, num_classes=5)\n",
    "    \n",
    "    return torch.stack(reviews), overalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d090f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(data, batch_size=32, collate_fn=pad_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07db45b",
   "metadata": {},
   "source": [
    "## 3. Model\n",
    "\n",
    "Model implementation inspired by Alladin Persson's [movie](https://www.youtube.com/watch?v=U0s0f995w14). [GitHub repo](https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/more_advanced/transformer_from_scratch/transformer_from_scratch.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0ce34fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        \n",
    "        assert self.head_dim * heads == embed_size, 'Embed size needs to be divisible by heads'\n",
    "        \n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        \n",
    "        self.fc_out = nn.Linear(heads*self.head_dim, embed_size)\n",
    "        \n",
    "    def forward(self, values, keys, queries, mask):\n",
    "        N = queries.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "        \n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = queries.reshape(N, query_len, self.heads, self.head_dim)\n",
    "        \n",
    "        values = self.values(values) \n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "        \n",
    "        energy = torch.einsum('nqhd, nkhd->nhqk', [queries, keys])\n",
    "        \n",
    "        if mask is not None:\n",
    "            # if mask at same point is 0 - shitdown this point - set to -inf, in softmax it will be 0\n",
    "            energy = energy.masked_fill(mask == 0, -1e20)\n",
    "            \n",
    "        attention = torch.softmax(energy / (self.embed_size**(1/2)), dim=3)\n",
    "        \n",
    "        # attention shape: N, heads, query_len, key_len\n",
    "        # values shape: N, value_len, heads, head_dim\n",
    "        # out shape: N, query_len, heads, head_dim\n",
    "        out = torch.einsum('nhql, nlhd->nqhd', [attention, values])\n",
    "        \n",
    "        out = out.reshape(N, query_len, self.heads*self.head_dim)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da2411bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion*embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion*embed_size, embed_size)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, value, key, queries, mask):\n",
    "        attention = self.attention(value, key, queries, mask)\n",
    "        \n",
    "        x = self.dropout(self.norm1(attention + queries))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd1e6c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, max_length, heads, device, forward_expansion, dropout):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(max_length*embed_size, max_length*forward_expansion)\n",
    "        self.fc2 = nn.Linear(max_length*forward_expansion, max_length)\n",
    "        \n",
    "        self.fc_out = nn.Linear(max_length, 5)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        N, seq_len = x.shape\n",
    "\n",
    "        positions = torch.arange(0, seq_len).expand(N, seq_len).to(self.device)\n",
    "\n",
    "        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x, x, mask)\n",
    "\n",
    "        # x shape: N, max_length, embed_size\n",
    "        # flat x\n",
    "        x = x.reshape(N, -1)\n",
    "        \n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        \n",
    "        # out shape: N, 5\n",
    "        out = self.fc_out(x)\n",
    "        \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ed3ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23d9fb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    vocab_size=cfg.vocab_size, \n",
    "    embed_size=cfg.embed_size, \n",
    "    num_layers=cfg.num_layers, \n",
    "    max_length=cfg.max_length,\n",
    "    heads=cfg.heads, \n",
    "    device=device, \n",
    "    forward_expansion=4, \n",
    "    dropout=0.25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon-rev-sent-analysis",
   "language": "python",
   "name": "amazon-rev-sent-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
