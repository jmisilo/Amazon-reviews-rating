{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d52ecdb6",
   "metadata": {},
   "source": [
    "## 1. Import depedencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4cb07cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import functools\n",
    "import operator \n",
    "import json\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from dataclasses import dataclass\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a34114",
   "metadata": {},
   "source": [
    "## 2. Data works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f52ef080",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "        self.vocab = {\n",
    "            '<unk>': 0,\n",
    "            '<pad>': 1,\n",
    "            '<sos>': 2,\n",
    "            '<eos>': 3\n",
    "        }\n",
    "        \n",
    "        self.build_vocab()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        assert type(index) in [str, int], 'Index type must be string or int'\n",
    "        \n",
    "        if isinstance(index, str):\n",
    "            try:\n",
    "                return self.vocab[index]\n",
    "            \n",
    "            except KeyError:\n",
    "                return self.vocab['<unk>']\n",
    "        \n",
    "        elif isinstance(index, int):\n",
    "            try:\n",
    "                return list(self.vocab.keys())[list(self.vocab.values()).index(index)]\n",
    "            except (KeyError,ValueError):\n",
    "                return self[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def append_word(self, word):\n",
    "        if not word in self.vocab:\n",
    "            self.vocab[word] = len(self)\n",
    "    \n",
    "    def build_vocab(self):\n",
    "        bag_of_words = sorted(list(set(self.data)))\n",
    "        \n",
    "        for word in bag_of_words:\n",
    "            self.append_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b906f444",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.path = 'data/Video_Games_5.json'\n",
    "        self.prep_path = 'data/preprocessed.csv'\n",
    "        \n",
    "        # if preprocessed data already exists - load it\n",
    "        if os.path.isfile(self.prep_path):\n",
    "            self.data = pd.read_csv(self.prep_path)[['reviewText', 'overall']]\n",
    "        \n",
    "        # else preprocess and save\n",
    "        else:\n",
    "            with open(self.path, 'r') as f:\n",
    "                lines = [json.loads(line.rstrip()) for line in f]\n",
    "                self.data = pd.DataFrame(lines)[['verified', 'reviewText', 'overall']]\n",
    "                self.data = self.data[self.data['verified']]\n",
    "\n",
    "            self.data = self.data.dropna()\n",
    "            self.data = self.data.reset_index(drop=True)\n",
    "            self.data = self.data.drop('verified', axis=1)\n",
    "\n",
    "            self.data['reviewText'] = self.data['reviewText'].apply(self.clean_data)\n",
    "            self.tok_lemma()\n",
    "\n",
    "            self.data.to_csv(self.prep_path)\n",
    "            \n",
    "        self.build_vocab()\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        assert type(index) == int, 'Index must be int'\n",
    "        \n",
    "        item = self.data.iloc[index]\n",
    "        text = str(item['reviewText']).split()\n",
    "        \n",
    "        for i, word in enumerate(text):\n",
    "            text[i] = self.Voc[word]\n",
    "            \n",
    "        return text, item['overall']\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_data(text):\n",
    "        if type(text) != str:\n",
    "            return '   '\n",
    "        \n",
    "        # lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # remove \\n signs\n",
    "        text = text.replace('\\n\\n\\n\\n', ' ').replace('\\n\\n\\n', ' ').replace('\\n\\n', ' ').replace('\\n', ' ')\n",
    "        \n",
    "        # remove url\n",
    "        text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # remove punctuations\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        return text \n",
    "    \n",
    "    def tok_lemma(self):\n",
    "        reviews = self.data['reviewText'].values\n",
    "\n",
    "        reviews = nlp.pipe(reviews, batch_size=128, n_process=3)\n",
    "\n",
    "        reviews = [' '.join([word.lemma_ for word in text if not word in stopwords and word.lemma_ and word.text.isalpha()]) for text in reviews]\n",
    "\n",
    "        self.data['reviewText'] = pd.Series(reviews) \n",
    "        \n",
    "    def build_vocab(self):\n",
    "        bag_of_words = self.data['reviewText'].apply(lambda x: str(x).split()).tolist()\n",
    "\n",
    "        bag_of_words = functools.reduce(operator.iconcat, bag_of_words, [])\n",
    "        \n",
    "        self.Voc = Vocabulary(bag_of_words)\n",
    "\n",
    "        self.Voc.build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b91f07a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = RevDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e734e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class cfg:\n",
    "    max_length = 310\n",
    "    batch_size = 32\n",
    "    lr = 3e-4\n",
    "    vocab_size = len(data.Voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d381485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lens = [len(data[i][0]) for i in range(len(data)) if len(data[i][0]) < 500]\n",
    "# plt.figure(figsize=(11, 8))\n",
    "# sns.histplot(data=lens, bins=15, kde=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "958c6e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(batch):\n",
    "    reviews = []\n",
    "    overalls = []\n",
    "    for i, (text, overall) in enumerate(batch):\n",
    "        \n",
    "        text_len = len(text)\n",
    "        \n",
    "        if text_len == cfg.max_length:\n",
    "            pass\n",
    "        elif text_len > cfg.max_length:\n",
    "            text = text[:cfg.max_length]\n",
    "        else:\n",
    "            pad_len = cfg.max_length - text_len\n",
    "            for j in range(pad_len):\n",
    "                # 1 - index of <pad> token in Vocabulary\n",
    "                text.append(1)\n",
    "        reviews.append(torch.Tensor(text).type(torch.int64))\n",
    "        overalls.append(int(overall) - 1)\n",
    "    \n",
    "    overalls = torch.LongTensor(overalls)\n",
    "    overalls = F.one_hot(overalls, num_classes=5)\n",
    "    \n",
    "    return torch.stack(reviews), overalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d090f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(data, batch_size=32, collate_fn=pad_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ce34fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon-rev-sent-analysis",
   "language": "python",
   "name": "amazon-rev-sent-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
